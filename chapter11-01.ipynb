{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.bpe import BPETokenizer\n",
    "tokenizer = BPETokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            0.5\n",
    "            * x\n",
    "            * (\n",
    "                1.0\n",
    "                + torch.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * torch.pow(x, 3.0)))\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.n_layer = 48\n",
    "        self.n_head = 25\n",
    "        self.n_embd = 1600\n",
    "        self.vocab_size = 50257\n",
    "        self.block_size = 1024\n",
    "        self.embd_pdrop = 0.1\n",
    "        self.resid_pdrop = 0.1\n",
    "        self.attn_pdrop = 0.1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3*config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropput = nn.Dropout(config.resid_pdrop)\n",
    "        self.register_buffer('bias', torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        hs = C // self.n_head\n",
    "        k = k.view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, hs).transpose(1, 2)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / np.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0,  float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v\n",
    "        y = y.transpose(1,2).contiguous().view(B,T,C)\n",
    "        y = self.resid_dropput(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CasualSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc = nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            act = GELU(),\n",
    "            dropout = nn.Dropout(config.resid_pdrop)\n",
    "        ))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp.dropout(self.mlp.c_proj(self.mlp.act(self.mlp.c_fc(self.ln_2(x)))))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2XL(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.block_size = config.block_size\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.embd_pdrop),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = nn.LayerNorm(config.n_embd),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        pos = torch.arange(0, t, dtype=torch.long).unsqueeze(0)\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1,logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        return logits, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "model=GPT2XL(config)\n",
    "num=sum(p.numel() for p in model.transformer.parameters())\n",
    "print(\"number of parameters: %.2fM\" % (num/1e6,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model_hf = GPT2LMHeadModel.from_pretrained('gpt2-xl')\n",
    "sd_hf = model_hf.state_dict()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd=model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight',\n",
    "              'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "for k in keys:\n",
    "    if any(k.endswith(w) for w in transposed):\n",
    "        # special treatment for Conv1D weights\n",
    "        with torch.no_grad():\n",
    "            sd[k].copy_(sd_hf[k].t())\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            sd[k].copy_(sd_hf[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def sample(idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        # if the text is more than 1024 tokens, trim it\n",
    "        if idx.size(1) <= config.block_size:\n",
    "            idx_cond = idx  \n",
    "        else:\n",
    "            idx_cond = idx[:, -config.block_size:]\n",
    "        # predict the logits for the index in sequence\n",
    "        logits, _ = model(idx_cond)\n",
    "        # pluck the logits at the final step; apply temperature \n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        # optionally crop the logits to only the top k options\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        if idx_next.item()==tokenizer.encoder.encoder['<|endoftext|>']:\n",
    "            break\n",
    "        # append new index to sequence \n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens, temperature=1., top_k=None):\n",
    "    if prompt == '':\n",
    "        x=torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt)\n",
    "    y = sample(x, max_new_tokens, temperature, top_k)\n",
    "    out = tokenizer.decode(y.squeeze())\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>Surely the work is done in Canada. If someone is translating Laato's In the Publishing House it's for us. I'm surprised ACM hasn't picked it up yet.\n",
      "\n",
      "\n",
      "In other news, I noticed this smattering of material after being all giddy about In the Biz.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"\"\n",
    "torch.manual_seed(42)\n",
    "generate(prompt, max_new_tokens=100, temperature=1.0, top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexington is the second largest city in the state of Kentucky, and the 903rd largest city in the nation, according to the U.S. Census. Its density of over 22,000 residents makes it one of the densest in the state. Besides its attractions of beauty, history, histitage, commercial and architectural sophistication, Lexington is also home to a community of artists and a strong music community. In fact, The Post outlined a growing list of musicians, including Edward Sharpe and The Magnetic Zeros, Dave Matthews Band and Ryan Adams\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Lexington is the second largest city in the state of Kentucky\"\n",
    "torch.manual_seed(42)\n",
    "generate(prompt, max_new_tokens=100, temperature=1.0,\n",
    "top_k=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mac-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
